{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.1 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rshwndsz/templates/blob/master/pl-colab/template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3ZAarhZX6bg"
      },
      "source": [
        "# PL_TEMPLATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNJxTTxoX7oQ"
      },
      "source": [
        "## 0a. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4FYNFxPXF2v"
      },
      "source": [
        "%%shell\n",
        "pip install pytorch-lightning > /dev/null 2>&1\n",
        "pip install neptune-client    > /dev/null 2>&1\n",
        "pip install torchmetrics      > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs5eE9JbXNP6"
      },
      "source": [
        "# STL\n",
        "import math\n",
        "import os\n",
        "import glob\n",
        "import logging\n",
        "import getpass\n",
        "from pathlib import Path\n",
        "\n",
        "# Numerical Python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Image processing\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.utils.data as D\n",
        "import torchvision as tv\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# Bells & Whistles\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_lightning.loggers.neptune import NeptuneLogger\n",
        "from pytorch_lightning.callbacks import (ModelCheckpoint,\n",
        "                                         EarlyStopping)\n",
        "\n",
        "# Misc\n",
        "from tqdm.notebook import tqdm\n",
        "import gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gi5nbpGXkZI"
      },
      "source": [
        "# Set up logging to file\n",
        "# https://stackoverflow.com/a/23681578\n",
        "\n",
        "logging.basicConfig(\n",
        "     filename='LOG.log',\n",
        "     level=logging.INFO, \n",
        "     format= '[%(asctime)s] %(levelname)8s - %(funcName)8s() - %(message)s',\n",
        "     datefmt='%H:%M:%S'\n",
        " )\n",
        "\n",
        "# Set up logging to console\n",
        "console = logging.StreamHandler()\n",
        "console.setLevel(logging.DEBUG)\n",
        "\n",
        "# Set a format which is simpler for console use\n",
        "formatter = logging.Formatter('%(levelname)8s - %(funcName)14s() - %(message)s')\n",
        "console.setFormatter(formatter)\n",
        "\n",
        "# Add the handler to the root logger\n",
        "logging.getLogger('').addHandler(console)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Test drive the logger\n",
        "logger.info(f\"\"\"\n",
        "            Torch: {torch.__version__}\n",
        "            Torchvision: {tv.__version__}\n",
        "            Pytorch Lightning: {pl.__version__} \n",
        "            Albumentations: {A.__version__}\n",
        "            \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9pIQ3CNX9lX"
      },
      "source": [
        "## 0b. Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udux5vfnj8tz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-xIcmFTYSmn"
      },
      "source": [
        "## 1. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuxgRWpWYSGs"
      },
      "source": [
        "# Sample dataset API\n",
        "class GenericImageDS(D.Dataset):\n",
        "    def __init__(self,\n",
        "                 root,\n",
        "                 image_glob=\"*.jpg\",\n",
        "                 train=True,\n",
        "                 transform=None,\n",
        "                 min_image_dim=256):\n",
        "        self.root = root\n",
        "        self.image_glob = image_glob\n",
        "        self.train = train\n",
        "        self.min_image_dim = min_image_dim\n",
        "\n",
        "        image_regex = os.path.join(self.root, self.image_glob)\n",
        "        self.image_paths = glob.glob(image_regex)\n",
        "        if not len(self.image_paths):\n",
        "            raise ValueError(f\"No image found using {image_regex}\")\n",
        "\n",
        "        self.transform = transform\n",
        "        # Default set of transforms if none are provided\n",
        "        if self.transform is None:\n",
        "            self.transform = A.Compose([\n",
        "                A.Resize(self.min_image_dim, self.min_image_dim, 4, True, 1),\n",
        "                A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0), p=1),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "        logger.info(f\"Total samples: {len(self.image_paths)}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def download(urls, destination_dir, force=False):\n",
        "        destination_dir = Path(destination_dir)\n",
        "\n",
        "        # Check validity of arguments\n",
        "        if not destination_dir.is_dir():\n",
        "            raise ValueError(\"Provide destination_dir\")\n",
        "        if urls is None:\n",
        "            raise ValueError(\"Provide URL(s)\")\n",
        "\n",
        "        # Download & Extract\n",
        "        for url in urls:\n",
        "            fname = download_file(url, destination_dir)\n",
        "            extract_file(fname, destination_dir)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.image_paths[index]\n",
        "        image = np.asarray(Image.open(image_path))\n",
        "        image = self.transform(image=image)[\"image\"]\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "\n",
        "# Test\n",
        "logger.setLevel(logging.DEBUG)\n",
        "# Write testing code here\n",
        "logger.setLevel(logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBxTMxZ9cs3K"
      },
      "source": [
        "## 2. Model Blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63_8iSRzcs3L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwmGy6RSaYzx"
      },
      "source": [
        "## 3. Lightning Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpgZFC7KgI2N"
      },
      "source": [
        "# Sample data module\n",
        "class ImageNet(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size=4, patch_size=11):\n",
        "        super().__init__()\n",
        "        pass\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"\n",
        "        For operations that might write to disk or \n",
        "        that need to be done only from a single process in distributed settings.\n",
        "        DO NOT use to assign state as it is called from a single process.\n",
        "        \"\"\"\n",
        "        URL    = \"\"\n",
        "        outdir = Path(\"./data/imagenet/\")\n",
        "\n",
        "        # Safely create nested directory\n",
        "        outdir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Download dataset\n",
        "        if not (outdir / \"dataset.mat\").exists():\n",
        "            gdown.download(URL, str(outdir / \"dataset.mat\"), quiet=False)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"For data operations on every GPU.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        pass\n",
        "        \n",
        "    def val_dataloader(self):\n",
        "        pass\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        pass\n",
        "\n",
        "    def teardown(self, stage=None):\n",
        "        \"\"\"Used to clean-up when run is finished\"\"\"\n",
        "        shutil.rmtree(\"./data/imagenet\")\n",
        "    \n",
        "    def visualize(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "# Test\n",
        "logger.setLevel(logging.DEBUG)\n",
        "# Write testing code here\n",
        "logger.setLevel(logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57jkQ5F5aVs_"
      },
      "source": [
        "class FinalNet(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(FinalNet, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # TODO\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def loss_function(self, preds, targets):\n",
        "        # TODO\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # TODO\n",
        "        raise NotImplementedError\n",
        "                  \n",
        "    def train_dataloader(self):\n",
        "        # TODO\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def val_dataloader(self):\n",
        "        # TODO\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, targets = batch\n",
        "        preds = self(inputs)\n",
        "        loss  = self.loss_function(preds, targets)\n",
        "\n",
        "        self.logger.experiment.log_metric('step_train_loss', loss)\n",
        "        return { 'loss': loss }\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "\n",
        "        self.logger.experiment.log_metric('epoch_train_loss', avg_loss)\n",
        "        \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, targets = batch\n",
        "        preds = self(inputs)\n",
        "        loss = self.loss_function(preds, targets)\n",
        "\n",
        "        self.logger.experiment.log_metric('step_val_loss', loss)\n",
        "        return { 'val_loss': loss }\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_val_loss = torch.stack([output['val_loss'] for output in outputs]).mean()\n",
        "\n",
        "        self.log('avg_val_loss', avg_val_loss)\n",
        "        self.logger.experiment.log_metric('epoch_val_loss', avg_val_loss)\n",
        "\n",
        "\n",
        "# Test\n",
        "logger.setLevel(logging.DEBUG)\n",
        "# Write testing code here\n",
        "logger.setLevel(logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdb6OMKha1N3"
      },
      "source": [
        "## 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvUXgrOha0op"
      },
      "source": [
        "hparams = {\n",
        "    'lr': 0.0001, \n",
        "    'batch_size': 4,\n",
        "    'max_epochs': 200,\n",
        "    'min_epochs': 10,\n",
        "    'check_val_every_n_epoch': 4,\n",
        "    'precision': 32,     # https://pytorch-lightning.readthedocs.io/en/latest/amp.html\n",
        "    'benchmark': True,\n",
        "    'deterministic': False,\n",
        "    'use_gpu': torch.cuda.is_available(),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-53FRnJxa3b8"
      },
      "source": [
        "# https://pytorch-lightning.readthedocs.io/en/latest/weights_loading.html?highlight=ModelCheckpoint\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    dirpath    = \"./checkpoints/\",\n",
        "    filename   = '{epoch:03d}__{avg_val_loss:.5f}',\n",
        "    save_top_k = 5,\n",
        "    monitor    = 'avg_val_loss',\n",
        "    mode       = 'min',\n",
        "    period     = 5\n",
        ")\n",
        "\n",
        "# https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html\n",
        "early_stop_callback = EarlyStopping(\n",
        "   monitor   = 'avg_val_loss',\n",
        "   min_delta = 1e-7,\n",
        "   patience  = 3,\n",
        "   verbose   = True,\n",
        "   mode      = 'min'\n",
        ")\n",
        "\n",
        "# https://docs.neptune.ai/api-reference/neptune/experiments/index.html#neptune.experiments.Experiment\n",
        "pl_logger = NeptuneLogger(\n",
        "    api_key         = CONSTANTS['API_TOKEN'],\n",
        "    project_name    = f\"\", # TODO\n",
        "    close_after_fit = True,\n",
        "    experiment_name = '',  # TODO\n",
        "    params          = hparams,\n",
        "    offline_model   = True,  # Comment to log into neptune.ai\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9yuf8HGa8E8"
      },
      "source": [
        "logger.setLevel(logging.INFO)\n",
        "pl.seed_everything(CONSTANTS['SEED'])\n",
        "\n",
        "dataset = Houston18DataModule(batch_size=hparams['batch_size'], \n",
        "                              patch_size=hparams['patch_size'])\n",
        "\n",
        "model   = FinalNet(hparams=hparams)\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    gpus                    = -1 if hparams['use_gpu'] else 0,\n",
        "    precision               = hparams['precision'],\n",
        "    gradient_clip_val       = hparams['gradient_clip_val'],\n",
        "    benchmark               = hparams['benchmark'],\n",
        "    deterministic           = hparams['deterministic'],\n",
        "    max_epochs              = hparams['max_epochs'],\n",
        "    min_epochs              = hparams['min_epochs'],\n",
        "    check_val_every_n_epoch = hparams['check_val_every_n_epoch'],\n",
        "    logger                  = pl_logger,\n",
        "    checkpoint_callback     = model_checkpoint,\n",
        "    callbacks               = [early_stop_callback],\n",
        ") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Qnj1zATbRcz"
      },
      "source": [
        "# 🐉\n",
        "trainer.fit(model, dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nKV4JG7bVKc"
      },
      "source": [
        "# Log model summary\n",
        "for chunk in [x for x in str(model).split('\\n')]:\n",
        "    neptune_logger.experiment.log_text('model_summary', str(chunk))\n",
        "\n",
        "# Which GPUs where used?\n",
        "gpu_list = [f'{i}:{torch.cuda.get_device_name(i)}' \n",
        "            for i in range(torch.cuda.device_count())] \n",
        "neptune_logger.experiment.log_text('GPUs used', ', '.join(gpu_list))\n",
        "\n",
        "# Log best 3 model checkpoints to Neptune\n",
        "for k in model_checkpoint.best_k_models.keys():\n",
        "    model_name = 'checkpoints/' + k.split('/')[-1]\n",
        "    neptune_logger.experiment.log_artifact(k, model_name)\n",
        "\n",
        "# Save last path\n",
        "last_model_path = f\"checkpoints/last_model--epoch={trainer.current_epoch}.ckpt\"\n",
        "trainer.save_checkpoint(last_model_path)\n",
        "neptune_logger.experiment.log_artifact(\n",
        "    last_model_path, \n",
        "    'checkpoints/' + last_model_path.split('/')[-1]\n",
        ")\n",
        "\n",
        "# Log score of the best model checkpoint\n",
        "neptune_logger.experiment.set_property(\n",
        "    'best_model_score', \n",
        "    model_checkpoint.best_model_score.tolist()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0hfgUpGc8KY"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loFyeV9QdAMG"
      },
      "source": [
        "### Get weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPgTX3xoc9mH"
      },
      "source": [
        "# Get Neptune API token\n",
        "from getpass import getpass\n",
        "api_token = getpass(\"Enter Neptune.ai API token: \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBsssHUVc-_H"
      },
      "source": [
        "# Initialize Neptune project\n",
        "import neptune\n",
        "from neptune import Session\n",
        "\n",
        "session = Session.with_default_backend(api_token=api_token)\n",
        "project = session.get_project(f\"\") # TODO\n",
        "experiment = project.get_experiments(id='')[0] # TODO\n",
        "experiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Tl1389dDnu"
      },
      "source": [
        "# Download checkpoint from Neptune\n",
        "artifact_path   = 'epoch=133-avg_val_loss=1.06.ckpt'\n",
        "artifact_name   = artifact_path.split('/')[-1]\n",
        "checkpoint_dir  = os.path.join('checkpoints', 'downloads')\n",
        "checkpoint_path = os.path.join(checkpoint_dir, artifact_name)\n",
        "\n",
        "experiment.download_artifact(path=artifact_path, destination_dir=checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep_t3K3QdOf8"
      },
      "source": [
        "### Load weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK0SBDmudNma"
      },
      "source": [
        "model = FinalNet.load_from_checkpoint(checkpoint_path=checkpoint_path)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbQiVlludLp7"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r8XHFogdMKq"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}